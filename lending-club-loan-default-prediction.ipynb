{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1090393,"sourceType":"datasetVersion","datasetId":608703}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/swapnil09/lending-club-loan-default-prediction?scriptVersionId=208946170\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Problem Statement\nLendingClub is a US peer-to-peer lending company, headquartered in San Francisco, California. It was the first peer-to-peer lender to register its offerings as securities with the Securities and Exchange Commission (SEC), and to offer loan trading on a secondary market. LendingClub is the world's largest peer-to-peer lending platform.\n\nSolving this case study will give us an idea about how real business problems are solved using EDA and Machine Learning. In this case study, we will also develop a basic understanding of risk analytics in banking and financial services and understand how data is used to minimise the risk of losing money while lending to customers.","metadata":{}},{"cell_type":"markdown","source":"# Business Understanding\nYou work for the LendingClub company which specialises in lending various types of loans to urban customers. When the company receives a loan application, the company has to make a decision for loan approval based on the applicant’s profile. Two types of risks are associated with the bank’s decision:\n\n* If the applicant is likely to repay the loan, then not approving the loan results in a loss of business to the company\n* If the applicant is not likely to repay the loan, i.e. he/she is likely to default, then approving the loan may lead to a financial loss for the company\n\nThe data given contains the information about past loan applicants and whether they ‘defaulted’ or not. The aim is to identify patterns which indicate if a person is likely to default, which may be used for takin actions such as denying the loan, reducing the amount of loan, lending (to risky applicants) at a higher interest rate, etc.\n\nWhen a person applies for a loan, there are two types of decisions that could be taken by the company:\n\n1. Loan accepted: If the company approves the loan, there are 3 possible scenarios described below:\n* Fully paid: Applicant has fully paid the loan (the principal and the interest rate)\n* Current: Applicant is in the process of paying the instalments, i.e. the tenure of the loan is not yet completed. These candidates are not labelled as 'defaulted'.\n* Charged-off: Applicant has not paid the instalments in due time for a long period of time, i.e. he/she has defaulted on the loan\n2. Loan rejected: The company had rejected the loan (because the candidate does not meet their requirements etc.). Since the loan was rejected, there is no transactional history of those applicants with the company and so this data is not available with the company (and thus in this dataset)","metadata":{}},{"cell_type":"markdown","source":"# Business Objectives\n\n* LendingClub is the largest online loan marketplace, facilitating personal loans, business loans, and financing of medical procedures. Borrowers can easily access lower interest rate loans through a fast online interface.\n* Like most other lending companies, lending loans to ‘risky’ applicants is the largest source of financial loss (called credit loss). The credit loss is the amount of money lost by the lender when the borrower refuses to pay or runs away with the money owed. In other words, borrowers who defaultcause the largest amount of loss to the lenders. In this case, the customers labelled as 'charged-off' are the 'defaulters'.\n* If one is able to identify these risky loan applicants, then such loans can be reduced thereby cutting down the amount of credit loss. Identification of such applicants using EDA and machine learning is the aim of this case study.\n* In other words, the company wants to understand the driving factors (or driver variables) behind loan default, i.e. the variables which are strong indicators of default. The company can utilise this knowledge for its portfolio and risk assessment.\n* To develop your understanding of the domain, you are advised to independently research a little about risk analytics (understanding the types of variables and their significance should be enough).","metadata":{}},{"cell_type":"markdown","source":"# Data Description\n\nHere is the information on this particular data set:\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th></th>\n      <th>LoanStatNew</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>loan_amnt</td>\n      <td>The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>term</td>\n      <td>The number of payments on the loan. Values are in months and can be either 36 or 60.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>int_rate</td>\n      <td>Interest Rate on the loan</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>installment</td>\n      <td>The monthly payment owed by the borrower if the loan originates.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>grade</td>\n      <td>LC assigned loan grade</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>sub_grade</td>\n      <td>LC assigned loan subgrade</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>emp_title</td>\n      <td>The job title supplied by the Borrower when applying for the loan.*</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>emp_length</td>\n      <td>Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>home_ownership</td>\n      <td>The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>annual_inc</td>\n      <td>The self-reported annual income provided by the borrower during registration.</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>verification_status</td>\n      <td>Indicates if income was verified by LC, not verified, or if the income source was verified</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>issue_d</td>\n      <td>The month which the loan was funded</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>loan_status</td>\n      <td>Current status of the loan</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>purpose</td>\n      <td>A category provided by the borrower for the loan request.</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>title</td>\n      <td>The loan title provided by the borrower</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>zip_code</td>\n      <td>The first 3 numbers of the zip code provided by the borrower in the loan application.</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>addr_state</td>\n      <td>The state provided by the borrower in the loan application</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>dti</td>\n      <td>A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>earliest_cr_line</td>\n      <td>The month the borrower's earliest reported credit line was opened</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>open_acc</td>\n      <td>The number of open credit lines in the borrower's credit file.</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>pub_rec</td>\n      <td>Number of derogatory public records</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>revol_bal</td>\n      <td>Total credit revolving balance</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>revol_util</td>\n      <td>Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>total_acc</td>\n      <td>The total number of credit lines currently in the borrower's credit file</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>initial_list_status</td>\n      <td>The initial listing status of the loan. Possible values are – W, F</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>application_type</td>\n      <td>Indicates whether the loan is an individual application or a joint application with two co-borrowers</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>mort_acc</td>\n      <td>Number of mortgage accounts.</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>pub_rec_bankruptcies</td>\n      <td>Number of public record bankruptcies</td>\n    </tr>\n  </tbody>\n</table>\n\n---\n----","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import pointbiserialr\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, classification_report,roc_auc_score, \n                             roc_curve, auc,ConfusionMatrixDisplay, RocCurveDisplay)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE \nfrom imblearn.over_sampling import BorderlineSMOTE \nfrom imblearn.over_sampling import ADASYN  \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Collection","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/lending-club-dataset/lending_club_loan_two.csv\") \ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Missing Values Check","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def nulls_summary_table(df):\n    \"\"\"\n    Returns a summary table showing null value counts and percentage\n    \n    Parameters:\n    df (DataFrame): Dataframe to check\n    \n    Returns:\n    null_values (DataFrame)\n    \"\"\"\n    null_values = pd.DataFrame(df.isnull().sum())\n    null_values[1] = null_values[0]*100/len(df)\n    null_values.columns = ['null_count','null_pct']\n    return null_values\n\nnulls_summary_table(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df = df.copy()\ndrop_df = drop_df.dropna()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nulls_summary_table(drop_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nThe goal is to understand the data,look for important variables","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the occurrences of each loan status\nstatus_counts = drop_df['loan_status'].value_counts()\n\n# Create a bar chart\nplt.bar(status_counts.index, status_counts.values)\nplt.xlabel('Loan Status')\nplt.ylabel('Count')\nplt.title('Loan Status Distribution')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"term_counts = drop_df['term'].value_counts()\n\n# Create a bar chart\nplt.bar(term_counts.index, term_counts.values)\nplt.xlabel('Terms')\nplt.ylabel('Count')\nplt.title('Loan Terms Distribution')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.violinplot(drop_df['loan_amnt'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.boxplot(drop_df['loan_amnt'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.heatmap(drop_df.corr(numeric_only=True), annot=True, cmap='viridis')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(x = drop_df['installment'], y = drop_df['loan_amnt'], hue=drop_df['loan_status'], alpha=0.5)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.boxplot(x='loan_status', y='loan_amnt', data=drop_df)\nplt.title('Loan Amount Distribution by Loan Status')\nplt.xlabel('Loan Status')\nplt.ylabel('Loan Amount')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.boxplot(x='loan_status', y='installment', data=drop_df)\nplt.title('Installments Distribution by Loan Status')\nplt.xlabel('Loan Status')\nplt.ylabel('Installments')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(data=drop_df, x='loan_amnt', hue='loan_status', kde=True, multiple='stack')\nplt.xlabel('Loan Amount')\nplt.ylabel('Frequency')\nplt.title('Distribution of Loan Amount by Loan Status')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(data=drop_df, x='installment', hue='loan_status', kde=True, multiple='stack')\nplt.xlabel('Installment')\nplt.ylabel('Frequency')\nplt.title('Distribution of Installments by Loan Status')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.groupby(by='loan_status')['loan_amnt'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**grade & sub_grade**\n* grade: LC assigned loan grade\n* sub_grade: LC assigned loan subgrade\nLet's explore the Grade and SubGrade columns that LendingClub attributes to the loans.\n\nWhat are the unique possible grade & sub_grade?","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\n\nplt.subplot(2, 2, 1)\ngrade = sorted(drop_df.grade.unique().tolist())\nsns.countplot(x='grade', data=drop_df, hue='loan_status', order=grade)\n\nplt.subplot(2, 2, 2)\nsub_grade = sorted(drop_df.sub_grade.unique().tolist())\ng = sns.countplot(x='sub_grade', data=drop_df, hue='loan_status', order=sub_grade)\ng.set_xticklabels(g.get_xticklabels(), rotation=90);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **term, home_ownership, verification_status & purpose**\n* term: The number of payments on the loan. Values are in months and can be either 36 or 60.\n* home_ownership: The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n* verification_status: Indicates if income was verified by LC, not verified, or if the income source was verified\n* purpose: A category provided by the borrower for the loan request.","metadata":{}},{"cell_type":"code","source":"drop_df['home_ownership'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 20))\n\nplt.subplot(4, 2, 1)\nsns.countplot(data=drop_df, x='home_ownership', hue='loan_status')\nplt.title('Home Ownership by Loan Status')\nplt.xlabel('Home Ownership Type')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')\n\n\nplt.subplot(4, 2, 2)\nsns.countplot(data=drop_df, x='term', hue='loan_status')\nplt.title('Term by Loan Status')\nplt.xlabel('Term Type')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')\n\n\nplt.subplot(4, 2, 3)\nsns.countplot(data=drop_df, x='verification_status', hue='loan_status')\nplt.title('Verfication Status by Loan Status')\nplt.xlabel('Verfication Status')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')\n\n\nplt.subplot(4, 2, 4)\nsns.countplot(data=drop_df, x='purpose', hue='loan_status')\nplt.title('Purpose by Loan Status')\nplt.xlabel('Purpose')\nplt.ylabel('Count')\nplt.xticks(rotation=90)  # Rotate x-axis labels if needed\nplt.legend(title='Loan Status')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.loc[drop_df['home_ownership']=='OTHER', 'loan_status'].value_counts()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **int_rate & annual_inc**\n* int_rate: Interest Rate on the loan\n* annual_inc: The self-reported annual income provided by the borrower during registration","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 20))\n\nplt.subplot(4, 2, 1)\nsns.histplot(data=drop_df, x='int_rate', hue='loan_status', bins =20, multiple=\"stack\", kde=True)\nplt.title('Interest Rate by Loan Status')\nplt.xlabel('Interest Rate')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')\n\n\nplt.subplot(4, 2, 2)\nsns.histplot(data=drop_df, x='annual_inc', hue='loan_status', bins =50, multiple=\"stack\", kde=True)\nplt.title('Annual Income by Loan Status')\nplt.xlabel('Annual Income')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(data=drop_df[drop_df['annual_inc']<= 250000], x='annual_inc', hue='loan_status', bins =50, multiple=\"stack\", kde=True)\nplt.title('Annual Income(<= 250000) by Loan Status')\nplt.xlabel('Annual Income')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print((drop_df[drop_df.annual_inc >= 250000].shape[0] / drop_df.shape[0]) * 100)\nprint((drop_df[drop_df.annual_inc >= 1000000].shape[0] / drop_df.shape[0]) * 100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* **only 1.05% borrowers have annual income greater than 250000 and only 0.018% people have annual income greater than 1000000.** ","metadata":{}},{"cell_type":"code","source":"drop_df.loc[drop_df.annual_inc >= 1000000, 'loan_status'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.loc[drop_df.annual_inc >= 250000, 'loan_status'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Loans with giher interest rates are more likely to be unpaid.\n* Only 61 borrowers have annual income greater than 1 Million and 3538 borrowers have annual income greater than 250K","metadata":{}},{"cell_type":"markdown","source":"## emp_title & emp_length\n* emp_title: The job title supplied by the Borrower when applying for the loan.\n* emp_length: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.","metadata":{}},{"cell_type":"code","source":"drop_df['emp_title'].value_counts()[:20]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 12))\n\nplt.subplot(2, 2, 1)\norder = ['< 1 year', '1 year', '2 years', '3 years', '4 years', '5 years', \n          '6 years', '7 years', '8 years', '9 years', '10+ years',]\ng = sns.countplot(x='emp_length', data=drop_df, hue='loan_status', order=order)\ng.set_xticklabels(g.get_xticklabels(), rotation=90);\n\nplt.subplot(2, 2, 2)\nplt.barh(drop_df.emp_title.value_counts()[:30].index, drop_df.emp_title.value_counts()[:30])\nplt.title(\"The most 30 jobs title afforded a loan\")\nplt.tight_layout()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## title\n* title: The loan title provided by the borrower","metadata":{}},{"cell_type":"code","source":"drop_df.title.value_counts()[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will remove title column as we already have a purpose column.","metadata":{}},{"cell_type":"markdown","source":"## dti, open_acc, revol_bal, revol_util, & total_acc\n* dti: A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n* open_acc: The number of open credit lines in the borrower's credit file.\n* revol_bal: Total credit revolving balance\n* revol_util: Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n* total_acc: The total number of credit lines currently in the borrower's credit file","metadata":{}},{"cell_type":"code","source":"drop_df['closed_credit_line'] = drop_df.apply(lambda row: 0 if row['total_acc'] - row['open_acc'] == 0 else 1, axis=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df['closed_credit_line'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This means that 332345 borrowers have a atleast 1 credit line closed which says that there is a high chance of them not defaulting a loan as they have previous experience managing multiple credit lines and closing them by paying off the loan.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 20))\n\nplt.subplot(4, 2, 1)\nsns.histplot(data=drop_df, x='dti', hue='loan_status', bins =50, multiple=\"stack\", kde=True)\nplt.title('dti by Loan Status')\nplt.xlabel('Debt to Income Ratio')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')\n\n\nplt.subplot(4, 2, 2)\nsns.histplot(data=drop_df[drop_df['dti']<= 50], x='dti', hue='loan_status', bins =50, multiple=\"stack\", kde=True)\nplt.title('dti(<= 50) by Loan Status')\nplt.xlabel('Debt to Income Ratio')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.loc[drop_df['dti']>=50, 'loan_status'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 20))\n\nplt.subplot(4, 2, 1)\nsns.histplot(data=drop_df, x='open_acc', hue='loan_status', bins =50, multiple=\"stack\", kde=True)\nplt.title('Loan Status by The number of open credit lines')\nplt.xlabel('The number of open credit lines')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')\n\n\nplt.subplot(4, 2, 2)\nsns.histplot(data=drop_df, x='total_acc', hue='loan_status', bins =50, multiple=\"stack\", kde=True)\nplt.title('Loan Status by The total number of credit lines')\nplt.xlabel('The total number of credit lines')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(drop_df.shape)\nprint(drop_df[drop_df.open_acc > 40].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(drop_df.shape)\nprint(drop_df[drop_df.total_acc > 80].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(drop_df.shape)\nprint(drop_df[drop_df.revol_util > 120].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 20))\n\nplt.subplot(4, 2, 1)\nsns.histplot(data=drop_df, x='revol_util', hue='loan_status', bins =50, multiple=\"stack\", kde=True)\nplt.title('Loan Status by Revolving line utilization rate')\nplt.xlabel('Revolving line utilization rate')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')\n\n\nplt.subplot(4, 2, 2)\nsns.histplot(data=drop_df[drop_df['revol_util']<=120], x='revol_util', hue='loan_status', bins =50, multiple=\"stack\", kde=True)\nplt.title('Loan Status by Revolving line utilization rate(<= 120)')\nplt.xlabel('Revolving line utilization rate')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df[drop_df['revol_util'] > 200]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(drop_df.shape)\nprint(drop_df[drop_df.revol_bal > 250000].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 20))\n\nplt.subplot(4, 2, 1)\nsns.histplot(data=drop_df, x='revol_bal', hue='loan_status', bins =50, multiple=\"stack\", kde=True)\nplt.title('Loan Status by Revolving balance')\nplt.xlabel('Revolving balance')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')\n\n\nplt.subplot(4, 2, 2)\nsns.histplot(data=drop_df[drop_df['revol_bal'] <= 250000], x='revol_bal', hue='loan_status', bins =50, multiple=\"stack\", kde=True)\nplt.title('Loan Status by Revolving balance(<= 250000)')\nplt.xlabel('Revolving balance')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.loc[drop_df.revol_bal > 250000, 'loan_status'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* It seems that the smaller the dti the more likely that the loan will not be paid.\n* Only 217 borrower have more than 40 open credit lines.\n* Only 266 borrower have more than 80 credit line in the borrower credit file.","metadata":{}},{"cell_type":"markdown","source":"## Correlation between Loan Status and Numerical features","metadata":{}},{"cell_type":"code","source":"numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\nprint(numerical_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df['loan_status'] = drop_df.loan_status.map({'Fully Paid':1, 'Charged Off':0})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate Point-Biserial correlation for each numerical feature\nfor col in numerical_features:\n    correlation, p_value = pointbiserialr(drop_df[col], drop_df['loan_status'])\n    print(f\"{col}: correlation = {correlation}, p-value = {p_value}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Strongest Predictors: int_rate (interest rate) and dti (debt-to-income ratio) have the highest correlation with loan default and are statistically significant. They may play important roles in any predictive model.\n* Weak or Negligible Predictors: Features like revol_bal, total_acc, and pub_rec_bankruptcies show little to no correlation with default, suggesting they may not be valuable in predicting default risk.","metadata":{}},{"cell_type":"code","source":"for col in numerical_features:\n    plt.figure(figsize=(8, 4))\n    sns.boxplot(x='loan_status', y=col, data=drop_df)\n    plt.title(f'Distribution of {col} by loan status')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data PreProcessing\n**Objectives** -\n* Convert categorical features into dummy variables.\n* Detect outliers and remove them or winsorize them.\n* Remove unnecessary or repetitive features.","metadata":{}},{"cell_type":"code","source":"print(f\"The Length of the data: {drop_df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.emp_title.nunique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* There are too many titles so we can't create dummies for these values so we'll have to drop this column.","metadata":{}},{"cell_type":"code","source":"drop_df.drop('emp_title', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.emp_length.unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for year in drop_df.emp_length.unique():\n    print(f\"{year} in this position:\")\n    print(f\"{drop_df[drop_df.emp_length == year].loan_status.value_counts(normalize=True)}\")\n    print('==========================================')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Charge off rates are extremely similar across all employment lengths. So we are going to drop the emp_length column.","metadata":{}},{"cell_type":"code","source":"drop_df.drop('emp_length', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.title.value_counts().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.purpose.value_counts().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The title column is simply a string subcategory/description of the purpose column. So we are going to drop the title column.","metadata":{}},{"cell_type":"code","source":"drop_df.drop('title', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.mort_acc.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pub_rec(number):\n    if number == 0.0:\n        return 0\n    else:\n        return 1\n    \ndef mort_acc(number):\n    if number == 0.0:\n        return 0\n    elif number >= 1.0:\n        return 1\n    else:\n        return number\n    \ndef pub_rec_bankruptcies(number):\n    if number == 0.0:\n        return 0\n    elif number >= 1.0:\n        return 1\n    else:\n        return number","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df['pub_rec'] = drop_df.pub_rec.apply(pub_rec)\ndrop_df['mort_acc'] = drop_df.mort_acc.apply(mort_acc)\ndrop_df['pub_rec_bankruptcies'] = drop_df.pub_rec_bankruptcies.apply(pub_rec_bankruptcies)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.mort_acc.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df['pub_rec'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df['pub_rec_bankruptcies'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 30))\n\nplt.subplot(6, 2, 1)\nsns.countplot(x='pub_rec', data=drop_df, hue='loan_status')\n\nplt.subplot(6, 2, 2)\nsns.countplot(x='initial_list_status', data=drop_df, hue='loan_status')\n\nplt.subplot(6, 2, 3)\nsns.countplot(x='application_type', data=drop_df, hue='loan_status')\n\nplt.subplot(6, 2, 4)\nsns.countplot(x='mort_acc', data=drop_df, hue='loan_status')\n\nplt.subplot(6, 2, 5)\nsns.countplot(x='pub_rec_bankruptcies', data=drop_df, hue='loan_status')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.dropna(inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Categorical Variables","metadata":{}},{"cell_type":"code","source":"print([column for column in drop_df.columns if drop_df[column].dtype == object])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.term.unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"term_values = {' 36 months': 36, ' 60 months': 60}\ndrop_df['term'] = drop_df.term.map(term_values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.term.unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.grade.unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_df.sub_grade.unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We know that grade is just a sub feature of sub_grade, So we are goinig to drop it.","metadata":{}},{"cell_type":"code","source":"drop_df.drop('grade', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dummies = ['sub_grade', 'verification_status', 'purpose', 'initial_list_status', \n           'application_type', 'home_ownership']\nfinal_df = pd.get_dummies(drop_df, columns=dummies, drop_first=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Address**\n* We are going to feature engineer a zip code column from the address in the data set. Create a column called 'zip_code' that extracts the zip code from the address column.","metadata":{}},{"cell_type":"code","source":"final_df.address.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df['zip_code'] = final_df.address.apply(lambda x: x[-5:])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.zip_code.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.zip_code.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df = pd.get_dummies(final_df, columns=['zip_code'], drop_first=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.drop('address', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**issue_d**\n* This would be data leakage, we wouldn't know beforehand whether or not a loan would be issued when using our model, so in theory we wouldn't have an issue_date, drop this feature.","metadata":{}},{"cell_type":"code","source":"final_df.drop('issue_d', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert 'earliest_cr_line' to datetime format\nfinal_df['earliest_cr_line'] = pd.to_datetime(final_df['earliest_cr_line'], errors='coerce')\n\n# Extract the year from 'earliest_cr_line'\nfinal_df['earliest_cr_line'] = final_df['earliest_cr_line'].dt.year","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.earliest_cr_line.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"code","source":"w_p = final_df.loan_status.value_counts()[0] / final_df.shape[0]\nw_n = final_df.loan_status.value_counts()[1] / final_df.shape[0]\n\nprint(f\"Weight of positive values {w_p}\")\nprint(f\"Weight of negative values {w_n}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we can see there's a clear data imbalance and we'll have to address this issue to train our model by using techniques like SMOTE.","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(final_df, test_size=0.33, random_state=42)\n\nprint(train.shape)\nprint(test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Removing Outliers","metadata":{}},{"cell_type":"code","source":"print(train[train['dti'] <= 50].shape)\nprint(train.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train.shape)\ntrain = train[train['annual_inc'] <= 250000]\ntrain = train[train['dti'] <= 50]\ntrain = train[train['open_acc'] <= 40]\ntrain = train[train['total_acc'] <= 80]\ntrain = train[train['revol_util'] <= 120]\ntrain = train[train['revol_bal'] <= 250000]\nprint(train.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Normalizing the Data","metadata":{}},{"cell_type":"code","source":"X_train, y_train = train.drop('loan_status', axis=1), train.loan_status\nX_test, y_test = test.drop('loan_status', axis=1), test.loan_status","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.dtypes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Handling data Imbalance","metadata":{}},{"cell_type":"code","source":"w_p_train = y_train.value_counts()[0] / y_train.shape[0]\nw_n_train = y_train.value_counts()[1] / y_train.shape[0]\n\nprint(f\"Weight of positive values {w_p}\")\nprint(f\"Weight of negative values {w_n}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can clearly see that there's a huge imbalance between our classes and we need to address it so that our model is trained on sufficient data for good predictions.","metadata":{}},{"cell_type":"markdown","source":"## Synthetic Minority Oversampling (SMOTE)\nSynthetic Minority Oversampling (SMOTE) is an oversampling technique that creates synthetic data points. SMOTE address’ the core problem in oversampling. Oversampling creates duplicate datapoints whereas SMOTE slightly alters these data points.","metadata":{}},{"cell_type":"code","source":"smote = SMOTE(random_state = 42) \nX_smote, y_smote = smote.fit_resample(X_train,y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_smote.value_counts().plot.bar()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Borderline Smote\nThe idea behind borderline SMOTE is that we only want to use data that’s at risk of being misclassified as the data to be oversampled. In this case, we build a classifier to classify points as positive or negative. Then, for the data points we misclassify, we oversample those data points. This would hopefully train our algorithm to better recognize these difficult instances and correct for them.","metadata":{}},{"cell_type":"code","source":"bsmote = BorderlineSMOTE(random_state = 42) \n\nX_bsmote, y_bsmote = bsmote.fit_resample(X_train,y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_bsmote.value_counts().plot.bar()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Adaptive Synthetic Oversampling (ADASYN)\nThe idea behind AdaSyn is to use a weight distribution of our minority class. Essentially, we give higher weight to instances that are more difficult to learn and lower weight to instances that are easier to learn. AdaSyn is very similar to safe-level SMOTE, except there’s just a different way of computing the synthetic data points.","metadata":{}},{"cell_type":"code","source":"adasyn = ADASYN(random_state = 42)\nX_ada, y_ada = adasyn.fit_resample(X_train,y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_ada.value_counts().plot.bar()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we can see for all the three methods the values counts is almost similar but the values that have been added are different based on the different techniques used to calculate them so we'll try to use all three sets of training data in our model and see which one performs the best.","metadata":{}},{"cell_type":"markdown","source":"# Model Building\nWe'll have logistic Regression as our baseline model and then we'll use XGBOOST Classifier, Random Forest Classifier models for increasing our performance.","metadata":{}},{"cell_type":"code","source":"def print_score(true, pred, train=True):\n    if train:\n        clf_report = pd.DataFrame(classification_report(true, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(true, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(true, pred)}\\n\")\n        \n    elif train==False:\n        clf_report = pd.DataFrame(classification_report(true, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(true, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(true, pred)}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_smote = np.array(X_train).astype(np.float32)\ny_smote = np.array(y_train).astype(np.float32)\nX_bsmote = np.array(X_train).astype(np.float32)\ny_bsmote = np.array(y_train).astype(np.float32)\nX_ada = np.array(X_train).astype(np.float32)\ny_ada = np.array(y_train).astype(np.float32)\nX_test = np.array(X_test).astype(np.float32)\ny_test = np.array(y_test).astype(np.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"**Trying with Simple SMOTE**","metadata":{}},{"cell_type":"code","source":"base = LogisticRegression(max_iter=500)\nbase.fit(X_smote, y_smote)\n\ny_smote_pred = base.predict(X_smote)\ny_test_pred = base.predict(X_test)\n\nprint_score(y_smote, y_smote_pred, train=True)\nprint_score(y_test, y_test_pred, train=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Trying with Baseline SMOTE**","metadata":{}},{"cell_type":"code","source":"base_b = LogisticRegression(max_iter=500)\nbase_b.fit(X_bsmote, y_bsmote)\n\ny_bsmote_pred = base_b.predict(X_bsmote)\ny_test_pred = base_b.predict(X_test)\n\nprint_score(y_bsmote, y_bsmote_pred, train=True)\nprint_score(y_test, y_test_pred, train=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Trying with ADASYNC**","metadata":{}},{"cell_type":"code","source":"base_ada = LogisticRegression(max_iter=500)\nbase_ada.fit(X_ada, y_ada)\n\ny_ada_pred = base_ada.predict(X_ada)\ny_test_pred = base_ada.predict(X_test)\n\nprint_score(y_ada, y_ada_pred, train=True)\nprint_score(y_test, y_test_pred, train=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With all the three techniques the base model gives similar results on both train and test sets so we'll just go with simple SMOTE.","metadata":{}},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay.from_estimator(\n    base, X_test, y_test, \n    cmap='Blues', values_format='d', \n    display_labels=['Default', 'Fully-Paid']\n)\n\ndisp = RocCurveDisplay.from_estimator(base, X_test, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores_dict= {\n    'Logistic Regression':{\n        'Train': roc_auc_score(y_smote, base.predict(X_smote)),\n        'Test': roc_auc_score(y_test, base.predict(X_test)),\n    },\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators = 100)\nrfc.fit(X_smote, y_smote)\n\ny_smote_pred_rfc = rfc.predict(X_smote)\ny_test_pred_rfc = rfc.predict(X_test)\n\nprint_score(y_smote, y_smote_pred_rfc, train = True)\nprint_score(y_test, y_test_pred_rfc, train = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay.from_estimator(\n    rfc, X_test, y_test, \n    cmap='Blues', values_format='d', \n    display_labels=['Default', 'Fully-Paid']\n)\n\ndisp = RocCurveDisplay.from_estimator(rfc, X_test, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores_dict['Random Forest Classifier'] = {\n        'Train': roc_auc_score(y_smote, rfc.predict(X_smote)),\n        'Test': roc_auc_score(y_test, rfc.predict(X_test)),\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(use_label_encoder=False)\nxgb.fit(X_smote, y_smote)\n\ny_smote_pred_xgb = xgb.predict(X_smote)\ny_test_pred_xgb = xgb.predict(X_test)\n\nprint_score(y_smote, y_smote_pred_xgb, train = True)\nprint_score(y_test, y_test_pred_xgb, train = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay.from_estimator(\n    xgb, X_test, y_test, \n    cmap='Blues', values_format='d', \n    display_labels=['Default', 'Fully-Paid']\n)\n\ndisp = RocCurveDisplay.from_estimator(xgb, X_test, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores_dict['XGBoost Classifier'] = {\n        'Train': roc_auc_score(y_smote, xgb.predict(X_smote)),\n        'Test': roc_auc_score(y_test, xgb.predict(X_test)),\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comparing Model Performances","metadata":{}},{"cell_type":"code","source":"ml_models = {\n    'Random Forest': rfc, \n    'XGBoost': xgb, \n    'Logistic Regression': base\n}\n\nfor model in ml_models:\n    print(f\"{model.upper():{30}} roc_auc_score: {roc_auc_score(y_test, ml_models[model].predict(X_test)\n):.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q hvplot","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores_df = pd.DataFrame(scores_dict)\nscores_df.hvplot.barh(\n    width=500, height=400, \n    title=\"ROC Scores of ML Models\", xlabel=\"ROC Scores\", \n    alpha=0.4, legend='top'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can say that the best performing model was XGBoost based on the ROC-AUC Score.","metadata":{}}]}